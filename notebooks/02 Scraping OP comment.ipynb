{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âœ…Step 2: Scraping Reddit Recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. ðŸŽ¯Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import requests as r\n",
    "\n",
    "from scrapy import Selector\n",
    "\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "# Import our own modules\n",
    "sys.path.append(\"../scripts/\")\n",
    "import chadtools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ðŸŽ¯Load credentials and set up Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a function defined in our `chadtools.py` script, we can authenticate with the Reddit API using our own personal `credentials.json` file, and get a `dict` of headers to be used in all subsequent GET requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Authorization': 'bearer eyJhbGciOiJSUzI1NiIsImtpZCI6IlNIQTI1NjpzS3dsMnlsV0VtMjVmcXhwTU40cWY4MXE2OWFFdWFyMnpLMUdhVGxjdWNZIiwidHlwIjoiSldUIn0.eyJzdWIiOiJ1c2VyIiwiZXhwIjoxNzA2MzEzNDgwLjIwMjIwOCwiaWF0IjoxNzA2MjI3MDgwLjIwMjIwOCwianRpIjoiWFJIV0wzZVFJRk5jeDlwdUlCQk1ZQUsweWx6bnB3IiwiY2lkIjoibWhUbV82eEVUNzVkOWhmWkJrS0ZYQSIsImxpZCI6InQyXzE2ZmE0MiIsImFpZCI6InQyXzE2ZmE0MiIsImxjYSI6MTQ5MDI0NzcyNzAxMSwic2NwIjoiZUp5S1Z0SlNpZ1VFQUFEX193TnpBU2MiLCJmbG8iOjl9.BXMtK1jB7OokT0dydxm98MRCoBKoG80KEXNkkD0RhpkuZeR0Ti6MTlhInWVW80mCpJxjfWOGnkKBXzuHt9BmncUbJSp3BSG2RN_zBa_hqf4Ujm57aChwTnt-ZE_Gjm5dtF8KmapTphpseeyXjXUqfL6I4cZcH4Ld_H8GjQy9B4K51ehRx2Tg-jhl3tnHVsBnvuTp2VrUKflucN_xw-kFeg-ujXPu3rRuFLrtyWOD1yDlT3e1TW1LW18qcJV0PXxkIUxhgfofAf3t6R-YFnXOeeFF7qzIFHg_BoUAWGJ3cFmr3D3DAJKfMu6qv6mvVluW03FZB-92AWvz_HUVKdMZeg',\n",
       " 'User-Agent': 'LSE DS105A Recipe Scraping Project by zichengliu'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = r.Session()\n",
    "headers = chadtools.authenticate_and_get_headers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ðŸŽ¯Sending Get Request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the normal links in the posts dataframe with the API links for better efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../data/posts.json', orient='records', lines=True)\n",
    "# extracting permalink\n",
    "permalink_list = df['permalink'].tolist()\n",
    "\n",
    "# Convert to usable link\n",
    "usable_comment_links = []\n",
    "for link in permalink_list:\n",
    "    modified_link = link.replace(\"https://\", \"\")\n",
    "    usable_comment_links.append(\"https://oauth.\" + modified_link)\n",
    "\n",
    "df.insert(df.columns.get_loc(\"permalink\"), \"comment_link\", usable_comment_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can now navigate through the variable content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: FOLLOWING CODE IS TAKES 20MIN TO RUN, load from json file instead and skip this chunk if testing."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "all_contents = []\n",
    "for link in tqdm(df[\"comment_link\"]):\n",
    "    # Send a GET request to the specified link with the necessary headers\n",
    "    response = s.get(link, headers=headers)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the content from the response\n",
    "        # We only want element 1 because response returns data on Post (which we don't want) and Comment (which we want)\n",
    "        content = response.json()[1]\n",
    "        all_contents.append(content)\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        all_contents.append(None)\n",
    "    sleep(0.3) # add delay due to reddit's rate limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the result to a json file for testing, so that we don't have to scrape for 20 min every time.\n",
    "WARNING: will create a >100MB file, so don't commit it to github. Have added it to .gitignore"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('../data/comments.json', 'w') as f:\n",
    "    json.dump(all_contents, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ðŸŽ¯Navigating to OP's recipe comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open from json file if testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/comments.json', 'r') as f:\n",
    "    all_contents = json.load(f)\n",
    "\n",
    "len(all_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We realised that under [\"children\"] there exists other posts made by the OP which are not the recipe, i.e. replies to other commenters.\n",
    "***Assumption**: the longest comment made by the OP is most likely to be the one containing the actual recipe.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each dictionary in the list\n",
    "ingredient_comment_list = []\n",
    "for x in all_contents:\n",
    "    if x != None:\n",
    "        # Extract comments from each dictionary\n",
    "        comments = [comment[\"data\"][\"body\"] for comment in x[\"data\"][\"children\"]]\n",
    "\n",
    "        # Find the longest comment\n",
    "        if len(comments) != 0:\n",
    "            ingredient_comment = max(comments, key=len)\n",
    "        else:\n",
    "            ingredient_comment = \"\"\n",
    "        ingredient_comment_list.append(ingredient_comment)\n",
    "    else:\n",
    "        ingredient_comment_list.append(None)\n",
    "\n",
    "df.insert(df.columns.get_loc(\"comment_link\"), \"ingredient_comment\", ingredient_comment_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to a json file in preparation for data cleanup, which will be done in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('../data/posts_with_comments.json', orient='records', indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
