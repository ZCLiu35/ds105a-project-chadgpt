{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âœ…Step 1: Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. ðŸŽ¯Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import requests as r\n",
    "import langid\n",
    "\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from scrapy import Selector\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# import plotnine\n",
    "# import altair\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import our own modules\n",
    "sys.path.append(\"../scripts/\")\n",
    "import chadtools as chadtools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ðŸŽ¯Authenticate with Reddit API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a function defined in our `utils.py` script, we can authenticate with the Reddit API using our own `credentials.json` file, and get a `dict` of headers to be used in all subsequent GET requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Authorization': 'bearer eyJhbGciOiJSUzI1NiIsImtpZCI6IlNIQTI1NjpzS3dsMnlsV0VtMjVmcXhwTU40cWY4MXE2OWFFdWFyMnpLMUdhVGxjdWNZIiwidHlwIjoiSldUIn0.eyJzdWIiOiJ1c2VyIiwiZXhwIjoxNzAxODE5MzYyLjEwMTc3MSwiaWF0IjoxNzAxNzMyOTYyLjEwMTc3MSwianRpIjoieWFtSzBhaHhrYUs4Tkd4VlJnVk5zdWFGTVVCTHVRIiwiY2lkIjoiZmVpckFYYmVWakEzOFN3cVRQT05LdyIsImxpZCI6InQyX2hleXBhN2ZhIiwiYWlkIjoidDJfaGV5cGE3ZmEiLCJsY2EiOjE2MzkxMjU5ODA5NzQsInNjcCI6ImVKeUtWdEpTaWdVRUFBRF9fd056QVNjIiwiZmxvIjo5fQ.DUlCUjeW8PVbq_n3vjsOAB3epb4qUyDd9NbNDicvg3NwDHUTJwFXZ8oqQugBX64ioBbxIFF3Pv-18Z6v-LGa-Xj8L1FFaMd63cZlGLXX6QPlq1r7_k5FrfwRa94TJByOSgchJX8Dfi0z7XqvvAL36KM2YpKFwBYTpjY4KIKz3zn6JdFnWjQ0ItEF3a5WZngstci6G9FDdGoXvCa7L4iwicljUhiF2p_vPKPjL1tsyBsXEVa1w9042rCF3oyuPmL1coGJNuAx9_fCpy-o_HMol2TC0SSRi3KHPvJTXeTlDhbz1gNc18EoDzs0GRLOdaYsIhaXjZaKCLd47lXPRyfOYg',\n",
       " 'User-Agent': 'LSE DS105A Recipe Scraping Project by ilovedatasci'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = chadtools.authenticate_and_get_headers()\n",
    "headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ðŸŽ¯Sending our first request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = r.Session()\n",
    "BASE_ENDPOINT = \"https://oauth.reddit.com\"\n",
    "flair_name = ['Recipe', 'Dessert', 'Pasta', 'Poultry', 'Vegetarian', 'Drink', 'Beef', 'Pork', 'Seafood', 'Fruit\\Vegetarian']\n",
    "subreddit_name = 'recipes'\n",
    "flair_query = ' OR '.join(f'flair_name:\"{flair}\"' for flair in flair_name)\n",
    "\n",
    "# specify earliest time to search from\n",
    "specific_date_time = datetime(2020, 8, 31, 10, 59, 0)\n",
    "timestamp = int(specific_date_time.timestamp())\n",
    "\n",
    "params = {'q': flair_query,\n",
    "          'limit': 100,\n",
    "          'restrict_sr': 0,\n",
    "          'sort': 'new',\n",
    "          'timestamp': timestamp}\n",
    "\n",
    "response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/search\", params=params, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the data from all pages\n",
    "all_data_in_subreddit = []\n",
    "\n",
    "#page 01 data\n",
    "data = response.json()\n",
    "\n",
    "# Process the data from the first page\n",
    "all_data_in_subreddit.extend(data['data']['children'])\n",
    "\n",
    "# Continue paginating until there is no more data \n",
    "while 'after' in data['data'] and data['data']['after'] is not None:\n",
    "    after_id = data['data']['after']\n",
    "    params[\"after\"] = after_id\n",
    "    # response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/hot?limit=100&after={after_id}/\", params=params, headers=headers)\n",
    "    response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/search\", params=params, headers=headers)\n",
    "    print(f\"Requesting Page {len(all_data_in_subreddit) // params['limit'] + 1}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Process the data from the current page\n",
    "    all_data_in_subreddit.extend(data['data']['children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_data_in_subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will limit our search to 3 posts first, to test whether our GET request works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = r.Session()\n",
    "\n",
    "BASE_ENDPOINT = \"https://oauth.reddit.com\"\n",
    "flair_name = 'Recipe'\n",
    "subreddit_name = 'recipes'\n",
    "\n",
    "params = {'q': f'flair_name:\"{flair_name}\"',\n",
    "          'limit': 100,\n",
    "          'restrict_sr': 1,\n",
    "          'sort': 'new'}\n",
    "\n",
    "response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/search\", params=params, headers=headers)\n",
    "\n",
    "# response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try paginating 3 times first, before increasing the number of page or paginating to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting Page 2\n",
      "Requesting Page 3\n",
      "Requesting Page 4\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the data from all pages\n",
    "all_data = []\n",
    "\n",
    "#page 01 data\n",
    "data = response.json()\n",
    "\n",
    "# Process the data from the first page\n",
    "all_data.extend(data['data']['children'])\n",
    "\n",
    "# Continue paginating until there is no more data (or paginate for a set number of times)\n",
    "\n",
    "# while data['data']['after'] is not None:\n",
    "for i in range(3):\n",
    "    after_id = data['data']['after']\n",
    "    params[\"after\"] = after_id\n",
    "    # response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/hot?limit=100&after={after_id}/\", params=params, headers=headers)\n",
    "    response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/search\", params=params, headers=headers)\n",
    "    print(f\"Requesting Page {i+2}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Process the data from the current page\n",
    "    all_data.extend(data['data']['children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting Page 2\n",
      "Requesting Page 3\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the data from all pages\n",
    "all_data_in_subreddit = []\n",
    "\n",
    "#page 01 data\n",
    "data = response.json()\n",
    "\n",
    "# Process the data from the first page\n",
    "all_data_in_subreddit.extend(data['data']['children'])\n",
    "\n",
    "# Continue paginating until there is no more data (or paginate for a set number of times)\n",
    "\n",
    "while 'after' in data['data'] and data['data']['after'] is not None:\n",
    "    after_id = data['data']['after']\n",
    "    params[\"after\"] = after_id\n",
    "    # response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/hot?limit=100&after={after_id}/\", params=params, headers=headers)\n",
    "    response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/search\", params=params, headers=headers)\n",
    "    print(f\"Requesting Page {len(all_data_in_subreddit) // params['limit'] + 1}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Process the data from the current page\n",
    "    all_data_in_subreddit.extend(data['data']['children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data_in_subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ðŸŽ¯Saving the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Saving the data as a JSON file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/all_data_flair_is_recipe.json\", \"w\") as f:\n",
    "    json.dump(all_data_in_subreddit, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load the JSON file as a Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/all_data_flair_is_recipe.json\", \"r\") as file:\n",
    "    posts = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Create a dataframe of all posts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>permalink</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stir Fry Supreme â€“ Chives, cashews and Shrimp</td>\n",
       "      <td>1.701695e+09</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>59</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com/r/recipes/comments/18ajm70/stir_fry...</td>\n",
       "      <td>https://i.redd.it/6vrftswiz94c1.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Polish Krokiety - Mushroom &amp;amp; Sauerkraut Cr...</td>\n",
       "      <td>1.701517e+09</td>\n",
       "      <td>260</td>\n",
       "      <td>0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>260</td>\n",
       "      <td>17</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com/r/recipes/comments/1891x51/polish_k...</td>\n",
       "      <td>https://i.redd.it/lsazpcn8bv3c1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Festive Southern Jalapeno Pimento Cheese Dip</td>\n",
       "      <td>1.701263e+09</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>172</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com/r/recipes/comments/186osjd/festive_...</td>\n",
       "      <td>https://i.redd.it/i0lvs10aba3c1.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quick &amp;amp; Easy Nut Brittle</td>\n",
       "      <td>1.701206e+09</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>118</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com/r/recipes/comments/1866xrq/quick_ea...</td>\n",
       "      <td>https://i.redd.it/elfhdi81n53c1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Green Borshch</td>\n",
       "      <td>1.700952e+09</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>62</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com/r/recipes/comments/183vmzc/green_bo...</td>\n",
       "      <td>https://i.redd.it/s8aaslwrnk2c1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>Matcha Fudge Chocolate Chip Marble Cookies (Re...</td>\n",
       "      <td>1.680281e+09</td>\n",
       "      <td>1404</td>\n",
       "      <td>0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1404</td>\n",
       "      <td>28</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com/r/recipes/comments/127rf0v/matcha_f...</td>\n",
       "      <td>https://i.redd.it/5r1vvzc7t3ra1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Greek Yogurt Chicken Salad with Apples and Alm...</td>\n",
       "      <td>1.680043e+09</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com/r/recipes/comments/1254xfn/greek_yo...</td>\n",
       "      <td>https://i.redd.it/kbpojb4e5kqa1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Louisiana Crawfish Boil</td>\n",
       "      <td>1.680008e+09</td>\n",
       "      <td>1214</td>\n",
       "      <td>0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1214</td>\n",
       "      <td>56</td>\n",
       "      <td>True</td>\n",
       "      <td>reddit.com/r/recipes/comments/124nr2l/louisian...</td>\n",
       "      <td>https://i.redd.it/sgmiy9z18hqa1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Drums of heaven</td>\n",
       "      <td>1.680000e+09</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>82</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com/r/recipes/comments/124kk3b/drums_of...</td>\n",
       "      <td>https://i.redd.it/97e0rjrm2iqa1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Butter Chicken</td>\n",
       "      <td>1.679870e+09</td>\n",
       "      <td>1454</td>\n",
       "      <td>0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1454</td>\n",
       "      <td>33</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com/r/recipes/comments/123276t/butter_c...</td>\n",
       "      <td>https://i.redd.it/tj8z88nst5qa1.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>249 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title   created_utc   ups  \\\n",
       "0        Stir Fry Supreme â€“ Chives, cashews and Shrimp  1.701695e+09    59   \n",
       "1    Polish Krokiety - Mushroom &amp; Sauerkraut Cr...  1.701517e+09   260   \n",
       "2         Festive Southern Jalapeno Pimento Cheese Dip  1.701263e+09   172   \n",
       "3                         Quick &amp; Easy Nut Brittle  1.701206e+09   118   \n",
       "4                                        Green Borshch  1.700952e+09    62   \n",
       "..                                                 ...           ...   ...   \n",
       "244  Matcha Fudge Chocolate Chip Marble Cookies (Re...  1.680281e+09  1404   \n",
       "245  Greek Yogurt Chicken Salad with Apples and Alm...  1.680043e+09    24   \n",
       "246                            Louisiana Crawfish Boil  1.680008e+09  1214   \n",
       "247                                    Drums of heaven  1.680000e+09    82   \n",
       "248                                     Butter Chicken  1.679870e+09  1454   \n",
       "\n",
       "     downs  upvote_ratio  score  num_comments  is_original_content  \\\n",
       "0        0          0.90     59             6                False   \n",
       "1        0          0.97    260            17                False   \n",
       "2        0          0.95    172            11                False   \n",
       "3        0          0.95    118            11                False   \n",
       "4        0          0.96     62             9                False   \n",
       "..     ...           ...    ...           ...                  ...   \n",
       "244      0          0.97   1404            28                False   \n",
       "245      0          0.97     24             2                False   \n",
       "246      0          0.96   1214            56                 True   \n",
       "247      0          0.94     82             5                False   \n",
       "248      0          0.99   1454            33                False   \n",
       "\n",
       "                                             permalink  \\\n",
       "0    reddit.com/r/recipes/comments/18ajm70/stir_fry...   \n",
       "1    reddit.com/r/recipes/comments/1891x51/polish_k...   \n",
       "2    reddit.com/r/recipes/comments/186osjd/festive_...   \n",
       "3    reddit.com/r/recipes/comments/1866xrq/quick_ea...   \n",
       "4    reddit.com/r/recipes/comments/183vmzc/green_bo...   \n",
       "..                                                 ...   \n",
       "244  reddit.com/r/recipes/comments/127rf0v/matcha_f...   \n",
       "245  reddit.com/r/recipes/comments/1254xfn/greek_yo...   \n",
       "246  reddit.com/r/recipes/comments/124nr2l/louisian...   \n",
       "247  reddit.com/r/recipes/comments/124kk3b/drums_of...   \n",
       "248  reddit.com/r/recipes/comments/123276t/butter_c...   \n",
       "\n",
       "                                      url  \n",
       "0    https://i.redd.it/6vrftswiz94c1.jpeg  \n",
       "1     https://i.redd.it/lsazpcn8bv3c1.jpg  \n",
       "2    https://i.redd.it/i0lvs10aba3c1.jpeg  \n",
       "3     https://i.redd.it/elfhdi81n53c1.jpg  \n",
       "4     https://i.redd.it/s8aaslwrnk2c1.jpg  \n",
       "..                                    ...  \n",
       "244   https://i.redd.it/5r1vvzc7t3ra1.jpg  \n",
       "245   https://i.redd.it/kbpojb4e5kqa1.jpg  \n",
       "246   https://i.redd.it/sgmiy9z18hqa1.jpg  \n",
       "247   https://i.redd.it/97e0rjrm2iqa1.jpg  \n",
       "248   https://i.redd.it/tj8z88nst5qa1.jpg  \n",
       "\n",
       "[249 rows x 10 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts = pd.json_normalize(posts, max_level=0)\n",
    "\n",
    "df_posts['data'] = df_posts['data'].apply(lambda x: {} if pd.isna(x) else x)    # handle NaN values\n",
    "df_posts = pd.concat([df_posts.drop(['data'], axis=1), pd.json_normalize(df_posts['data'])], axis=1)\n",
    "\n",
    "selected_cols = ['title', 'created_utc', 'ups', 'downs', 'upvote_ratio', 'score', 'num_comments', 'is_original_content', 'permalink', 'url']\n",
    "\n",
    "df_posts = df_posts[selected_cols].copy()\n",
    "\n",
    "df_posts['permalink'] = \"reddit.com\" + df_posts['permalink']     # add prefix to each permalink \n",
    "\n",
    "df_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>permalink</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, created_utc, ups, downs, upvote_ratio, score, num_comments, is_original_content, permalink, url]\n",
       "Index: []"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_posts = df_posts[df_posts.duplicated(subset='permalink', keep=False)]\n",
    "duplicate_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       title   created_utc   ups  downs  \\\n",
      "3               Quick &amp; Easy Nut Brittle  1.701206e+09   118      0   \n",
      "4                              Green Borshch  1.700952e+09    62      0   \n",
      "18                           Al Pastor Tacos  1.699102e+09   304      0   \n",
      "24                              Lasagna Soup  1.698345e+09   324      0   \n",
      "30                       Quick ravioli salad  1.697988e+09    65      0   \n",
      "34                                 Apple Jam  1.697819e+09   103      0   \n",
      "38                        Beef Ribs two ways  1.697563e+09    39      0   \n",
      "39                           Classic Lasagna  1.697451e+09   183      0   \n",
      "45      Canadian Maple French Toast (Recipe)  1.696952e+09   117      0   \n",
      "47    Autumn Apple Cinnamon Cookies (Recipe)  1.696610e+09   312      0   \n",
      "65                      Quick Beef Rice Bowl  1.695222e+09   124      0   \n",
      "82   Apple Oatmeal Molasses Cookies (Recipe)  1.693594e+09   120      0   \n",
      "83                        French Toast Bites  1.693337e+09   950      0   \n",
      "120                      Mushroom Fried Rice  1.689364e+09   157      0   \n",
      "123                       Korean Corn Cheese  1.689226e+09   392      0   \n",
      "132               Refreshing Thai Corn Salad  1.688657e+09    20      0   \n",
      "136    Canadian Maple Syrup Cookies (Recipe)  1.688312e+09   426      0   \n",
      "138                  Healthy Thai Pork Salad  1.688277e+09    74      0   \n",
      "147                         Thai Green Curry  1.687507e+09   411      0   \n",
      "156             Ube Crinkle Cookies (Recipe)  1.686932e+09   301      0   \n",
      "161               Irish Oat Cookies (Recipe)  1.685822e+09   442      0   \n",
      "182            Hawaiian-Curry Fusion Burgers  1.683822e+09   151      0   \n",
      "208                   Creamy Beef Stroganoff  1.682304e+09   366      0   \n",
      "211             Lemon Honey Cookies (Recipe)  1.682096e+09   928      0   \n",
      "222              Ghanaian Peanut Butter Soup  1.681559e+09  1021      0   \n",
      "225                              Keema Curry  1.681343e+09   811      0   \n",
      "229                             Mutton Pulao  1.681252e+09    36      0   \n",
      "241                  Hummingbird Carrot Cake  1.680442e+09  1319      0   \n",
      "\n",
      "     upvote_ratio  score  num_comments  is_original_content  \\\n",
      "3            0.95    118            11                False   \n",
      "4            0.96     62             9                False   \n",
      "18           0.97    304            13                False   \n",
      "24           0.93    324            19                False   \n",
      "30           0.93     65             3                False   \n",
      "34           0.97    103            16                False   \n",
      "38           0.86     39             7                False   \n",
      "39           0.97    183             7                False   \n",
      "45           0.93    117             4                False   \n",
      "47           0.97    312            21                False   \n",
      "65           0.98    124             2                False   \n",
      "82           0.96    120            10                False   \n",
      "83           0.98    950            27                False   \n",
      "120          0.99    157             7                False   \n",
      "123          0.96    392            21                False   \n",
      "132          1.00     20             4                False   \n",
      "136          0.97    426            18                False   \n",
      "138          0.86     74             3                False   \n",
      "147          0.94    411            12                False   \n",
      "156          0.92    301             5                False   \n",
      "161          0.97    442            54                False   \n",
      "182          0.92    151            14                False   \n",
      "208          0.98    366            11                False   \n",
      "211          0.98    928            11                False   \n",
      "222          0.95   1021            73                False   \n",
      "225          0.97    811            30                False   \n",
      "229          0.96     36             3                False   \n",
      "241          0.99   1319            31                 True   \n",
      "\n",
      "                                             permalink  \\\n",
      "3    reddit.com/r/recipes/comments/1866xrq/quick_ea...   \n",
      "4    reddit.com/r/recipes/comments/183vmzc/green_bo...   \n",
      "18   reddit.com/r/recipes/comments/17nkyzf/al_pasto...   \n",
      "24   reddit.com/r/recipes/comments/17h34nk/lasagna_...   \n",
      "30   reddit.com/r/recipes/comments/17dvsbs/quick_ra...   \n",
      "34    reddit.com/r/recipes/comments/17cf204/apple_jam/   \n",
      "38   reddit.com/r/recipes/comments/17a3fc2/beef_rib...   \n",
      "39   reddit.com/r/recipes/comments/1792osc/classic_...   \n",
      "45   reddit.com/r/recipes/comments/174oam3/canadian...   \n",
      "47   reddit.com/r/recipes/comments/171gvoj/autumn_a...   \n",
      "65   reddit.com/r/recipes/comments/16nnh7c/quick_be...   \n",
      "82   reddit.com/r/recipes/comments/167f8cq/apple_oa...   \n",
      "83   reddit.com/r/recipes/comments/164rrpi/french_t...   \n",
      "120  reddit.com/r/recipes/comments/14zpxdq/mushroom...   \n",
      "123  reddit.com/r/recipes/comments/14ybtc7/korean_c...   \n",
      "132  reddit.com/r/recipes/comments/14scb5e/refreshi...   \n",
      "136  reddit.com/r/recipes/comments/14or7m6/canadian...   \n",
      "138  reddit.com/r/recipes/comments/14oge5b/healthy_...   \n",
      "147  reddit.com/r/recipes/comments/14gsf3l/thai_gre...   \n",
      "156  reddit.com/r/recipes/comments/14b0yb6/ube_crin...   \n",
      "161  reddit.com/r/recipes/comments/13znd8k/irish_oa...   \n",
      "182  reddit.com/r/recipes/comments/13es6z8/hawaiian...   \n",
      "208  reddit.com/r/recipes/comments/12x0998/creamy_b...   \n",
      "211  reddit.com/r/recipes/comments/12uagma/lemon_ho...   \n",
      "222  reddit.com/r/recipes/comments/12mzo2n/ghanaian...   \n",
      "225  reddit.com/r/recipes/comments/12k49da/keema_cu...   \n",
      "229  reddit.com/r/recipes/comments/12izwpz/mutton_p...   \n",
      "241  reddit.com/r/recipes/comments/129lg9n/hummingb...   \n",
      "\n",
      "                                     url  \n",
      "3    https://i.redd.it/elfhdi81n53c1.jpg  \n",
      "4    https://i.redd.it/s8aaslwrnk2c1.jpg  \n",
      "18   https://i.redd.it/o47egloxubyb1.jpg  \n",
      "24   https://i.redd.it/tqtpdzcjblwb1.jpg  \n",
      "30   https://i.redd.it/5u0g3lf7vrvb1.jpg  \n",
      "34   https://i.redd.it/did3ezcdudvb1.jpg  \n",
      "38   https://i.redd.it/gyucfgl7qsub1.jpg  \n",
      "39   https://i.redd.it/eji0f4kgijub1.png  \n",
      "45   https://i.redd.it/2vuvc2ij9etb1.png  \n",
      "47   https://i.redd.it/g5dkfmuyylsb1.png  \n",
      "65   https://i.redd.it/dp3olu6hdfpb1.jpg  \n",
      "82   https://i.redd.it/p28w2742volb1.jpg  \n",
      "83   https://i.redd.it/fjcbpynqo3lb1.jpg  \n",
      "120  https://i.redd.it/wa8p20j4hzbb1.jpg  \n",
      "123  https://i.redd.it/ajtdpv6v4obb1.jpg  \n",
      "132  https://i.redd.it/t3nle7hn3dab1.jpg  \n",
      "136  https://i.redd.it/2fa0leammk9b1.png  \n",
      "138  https://i.redd.it/91drizciqh9b1.jpg  \n",
      "147  https://i.redd.it/3scb9hot4q7b1.jpg  \n",
      "156  https://i.redd.it/9jaifby5ne6b1.jpg  \n",
      "161  https://i.redd.it/og5tubqcxu3b1.jpg  \n",
      "182  https://i.redd.it/moul7q3fs9za1.jpg  \n",
      "208  https://i.redd.it/4tic3jeiesva1.jpg  \n",
      "211  https://i.redd.it/q14qouu8o9va1.png  \n",
      "222  https://i.redd.it/d6obuqsuu2ua1.jpg  \n",
      "225  https://i.redd.it/60r2dmqqijta1.jpg  \n",
      "229  https://i.redd.it/2hsys6izybta1.jpg  \n",
      "241  https://i.redd.it/qxs7alol4hra1.jpg  \n",
      "28\n"
     ]
    }
   ],
   "source": [
    "# Function to check if text is in English\n",
    "def is_english(text):\n",
    "    language, confidence = langid.classify(text)\n",
    "    return language == 'en' and confidence > 0.00000000001  # Adjust confidence threshold as needed\n",
    "\n",
    "# Apply the function to filter rows\n",
    "filtered_df = df_posts[df_posts['title'].apply(is_english)]\n",
    "\n",
    "print(filtered_df)\n",
    "print(len(filtered_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_posts.to_csv('../data/duplicates.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Save dataframe as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.to_csv('../data/posts.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Specify the subreddit and flair\n",
    "subreddit_name = 'recipes'\n",
    "flair_name = 'recipe'  # Change this to the desired flair\n",
    "\n",
    "# Reddit API endpoint for searching posts in a subreddit\n",
    "url = f'https://www.reddit.com/r/{subreddit_name}/search.json'\n",
    "\n",
    "# Define parameters for the search query\n",
    "params = {\n",
    "    'q': f'flair_name:\"{flair_name}\"',\n",
    "    'restrict_sr': 'on',  # Restrict the search to the specified subreddit\n",
    "    'sort': 'new',       # Sort by new to get all posts\n",
    "    'syntax': 'cloudsearch'\n",
    "}\n",
    "\n",
    "# Make the API request\n",
    "response = s.get(url, params=params, headers={'User-agent': 'your_user_agent'})\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    data = response.json()\n",
    "    \n",
    "    # Get the number of posts\n",
    "    num_posts = data['data']['dist']\n",
    "    \n",
    "    print(f\"Number of posts with '{flair_name}' flair in r/{subreddit_name}: {num_posts}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds105",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
