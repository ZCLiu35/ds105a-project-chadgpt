{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âœ…Step 1: Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. ðŸŽ¯Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests as r\n",
    "\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from scrapy import Selector\n",
    "\n",
    "import plotnine\n",
    "import altair\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ðŸŽ¯Load credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loads the `credentials.json` file in each of our local repos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_file_path = \"../credentials.json\"\n",
    "\n",
    "# open the file and load the data into a variable\n",
    "with open(credentials_file_path, \"r\") as f:\n",
    "    credentials = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ðŸŽ¯Obtaining a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = r.Session()\n",
    "\n",
    "# Set up authentication parameters \n",
    "client_auth = r.auth.HTTPBasicAuth(credentials[\"app_client_id\"], credentials[\"app_client_secret\"])\n",
    "\n",
    "# Send, via HTTP POST, your Reddit username and password\n",
    "post_data = {\"grant_type\": \"password\",\n",
    "             \"username\": credentials[\"reddit_username\"],\n",
    "             \"password\": credentials[\"reddit_password\"]}\n",
    "\n",
    "# Reddit API requests that we self-identify ourselves in the User-Agent\n",
    "headers = {\"User-Agent\": f\"LSE DS105A Recipe Scraping Project by {credentials['reddit_username']}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'access_token': 'eyJhbGciOiJSUzI1NiIsImtpZCI6IlNIQTI1NjpzS3dsMnlsV0VtMjVmcXhwTU40cWY4MXE2OWFFdWFyMnpLMUdhVGxjdWNZIiwidHlwIjoiSldUIn0.eyJzdWIiOiJ1c2VyIiwiZXhwIjoxNzAxNDczNDAwLjczNzI1NiwiaWF0IjoxNzAxMzg3MDAwLjczNzI1NiwianRpIjoiZEhtUnNjaU5iMFpLZ1V0WWQ1cWoxR29Fa18zWGVRIiwiY2lkIjoiVFJicTdUNUZLby1kTU1iSk5vMTdEQSIsImxpZCI6InQyXzJpOWF4eDh3IiwiYWlkIjoidDJfMmk5YXh4OHciLCJsY2EiOjE1NDA4OTAxODI0NTUsInNjcCI6ImVKeUtWdEpTaWdVRUFBRF9fd056QVNjIiwiZmxvIjo5fQ.VYzpJd-QZvx9itt04ld8a32trAO6I4AdaqgwLhQ5wMomIyBd7jTvUkg_IdUDA6qGT45YHCqjfwNe6mvtvW8TuOAaQqss8OsEIHc5tGGKTt5iDGA1_4OwKrHxSB6L3etRp41i-ah0iPf5MWz7D4bC5xHd_av3Oo3I_Kjg1hTGmWUx8X9xAKkHesuMMR54Q1wL1FIHRmVKXMa6TLYXdWg5sbio1H1gJLZtsoehQixtB1SaLcrcSUrhoU2RKuyx8TcLzN3Qw5JRB3OVweF0fKNJGnSmv8NFchRSh3SMkAEbvLgZ7d5ol_nFws5HEljDM8GGWd29Eglf8U-1EvIp-blIfw',\n",
       " 'token_type': 'bearer',\n",
       " 'expires_in': 86400,\n",
       " 'scope': '*'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From Reddit's API documentation, this is the endpoint I need\n",
    "ACCESS_TOKEN_ENDPOINT = \"https://www.reddit.com/api/v1/access_token\"\n",
    "\n",
    "# Send a HTTP POST \n",
    "response = s.post(ACCESS_TOKEN_ENDPOINT, auth=client_auth, data=post_data, headers=headers)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save our token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_token = response.json()['access_token']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, all requests need to be followed by these HTTP HEADERS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Authorization\": f\"bearer {my_token}\",\n",
    "           \"User-Agent\": f\"LSE DS105A Recipe Scraping Project by {credentials['reddit_username']}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ðŸŽ¯Sending our first request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will limit our search to 3 posts first, to test whether our GET request works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_ENDPOINT = \"https://oauth.reddit.com\"\n",
    "flair_name = 'Recipe'\n",
    "subreddit_name = 'recipes'\n",
    "\n",
    "params = {'q': f'flair_name:\"{flair_name}\"',\n",
    "          'limit': 3,\n",
    "          'restrict_sr': 1,\n",
    "          'sort': 'new'}\n",
    "\n",
    "response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/search\", params=params, headers=headers)\n",
    "\n",
    "# response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try paginating 3 times first, before increasing the number of page or paginating to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting Page 2\n",
      "Requesting Page 3\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the data from all pages\n",
    "all_data = []\n",
    "\n",
    "#page 01 data\n",
    "data = response.json()\n",
    "\n",
    "# Process the data from the first page\n",
    "all_data.extend(data['data']['children'])\n",
    "\n",
    "# Continue paginating until there is no more data (or paginate for a set number of times)\n",
    "\n",
    "# while data['data']['after'] is not None:\n",
    "for i in range(2):\n",
    "    after_id = data['data']['after']\n",
    "    params[\"after\"] = after_id\n",
    "    # response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/hot?limit=100&after={after_id}/\", params=params, headers=headers)\n",
    "    response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/search\", params=params, headers=headers)\n",
    "    print(f\"Requesting Page {i+2}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Process the data from the current page\n",
    "    all_data.extend(data['data']['children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ðŸŽ¯Saving the data to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/all_data_flair_is_recipe.json\", \"w\") as f:\n",
    "    json.dump(all_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/all_data_flair_is_recipe.json\", \"r\") as file:\n",
    "    posts = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3791\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'data'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/Yuyao/Desktop/DS105/ds105a-project-chadgpt/notebooks/01 Data Scraping.ipynb Cell 23\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Yuyao/Desktop/DS105/ds105a-project-chadgpt/notebooks/01%20Data%20Scraping.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_posts \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mjson_normalize(posts, max_level\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Yuyao/Desktop/DS105/ds105a-project-chadgpt/notebooks/01%20Data%20Scraping.ipynb#X63sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df_posts[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_posts[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: {} \u001b[39mif\u001b[39;00m pd\u001b[39m.\u001b[39misna(x) \u001b[39melse\u001b[39;00m x)  \u001b[39m# Handle NaN values\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Yuyao/Desktop/DS105/ds105a-project-chadgpt/notebooks/01%20Data%20Scraping.ipynb#X63sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df_posts \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([df_posts\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), pd\u001b[39m.\u001b[39mjson_normalize(df_posts[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m])], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Yuyao/Desktop/DS105/ds105a-project-chadgpt/notebooks/01%20Data%20Scraping.ipynb#X63sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m#selected_cols = ['title', 'created_utc', 'ups', 'downs', 'upvote_ratio', 'score', 'num_comments', 'is_original_content', 'permalink', 'url']\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Yuyao/Desktop/DS105/ds105a-project-chadgpt/notebooks/01%20Data%20Scraping.ipynb#X63sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Yuyao/Desktop/DS105/ds105a-project-chadgpt/notebooks/01%20Data%20Scraping.ipynb#X63sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#df_posts = df_posts[selected_cols].copy()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Yuyao/Desktop/DS105/ds105a-project-chadgpt/notebooks/01%20Data%20Scraping.ipynb#X63sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Yuyao/Desktop/DS105/ds105a-project-chadgpt/notebooks/01%20Data%20Scraping.ipynb#X63sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m#df_posts = df_posts.reindex(sorted(df_posts.columns), axis=1)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3892\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3893\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mget_loc(key)\n\u001b[1;32m   3894\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3895\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'data'"
     ]
    }
   ],
   "source": [
    "df_posts = pd.json_normalize(posts, max_level=1)\n",
    "\n",
    "df_posts['data'] = df_posts['data'].apply(lambda x: {} if pd.isna(x) else x)  # Handle NaN values\n",
    "df_posts = pd.concat([df_posts.drop(['data'], axis=1), pd.json_normalize(df_posts['data'])], axis=1)\n",
    "\n",
    "\n",
    "#selected_cols = ['title', 'created_utc', 'ups', 'downs', 'upvote_ratio', 'score', 'num_comments', 'is_original_content', 'permalink', 'url']\n",
    "\n",
    "#df_posts = df_posts[selected_cols].copy()\n",
    "\n",
    "#df_posts = df_posts.reindex(sorted(df_posts.columns), axis=1)\n",
    "\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['title', 'created_utc', 'ups', 'downs', 'upvote_ratio', 'score',\\n       'num_comments', 'is_original_content', 'permalink', 'url'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/Yuyao/Desktop/DS105/ds105a-project-chadgpt/notebooks/01 Data Scraping.ipynb Cell 25\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Yuyao/Desktop/DS105/ds105a-project-chadgpt/notebooks/01%20Data%20Scraping.ipynb#X56sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#sort columns by name to make it easier for me to figure out what I want to keep\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Yuyao/Desktop/DS105/ds105a-project-chadgpt/notebooks/01%20Data%20Scraping.ipynb#X56sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#df_reddit = df_reddit.reindex(sorted(df_reddit.columns), axis=1)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Yuyao/Desktop/DS105/ds105a-project-chadgpt/notebooks/01%20Data%20Scraping.ipynb#X56sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m selected_cols \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcreated_utc\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mups\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdowns\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mupvote_ratio\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnum_comments\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mis_original_content\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpermalink\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39murl\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Yuyao/Desktop/DS105/ds105a-project-chadgpt/notebooks/01%20Data%20Scraping.ipynb#X56sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m df_reddit \u001b[39m=\u001b[39m df_reddit[selected_cols]\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Yuyao/Desktop/DS105/ds105a-project-chadgpt/notebooks/01%20Data%20Scraping.ipynb#X56sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m df_reddit\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3897\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3898\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 3899\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39m_get_indexer_strict(key, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m   3901\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3902\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6114\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6111\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   6112\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6114\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6116\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   6117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6118\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6175\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6173\u001b[0m     \u001b[39mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6174\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 6175\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6177\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m   6178\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['title', 'created_utc', 'ups', 'downs', 'upvote_ratio', 'score',\\n       'num_comments', 'is_original_content', 'permalink', 'url'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "df_reddit = (pd.json_normalize(data['data'], max_level=1))\n",
    "#sort columns by name to make it easier for me to figure out what I want to keep\n",
    "#df_reddit = df_reddit.reindex(sorted(df_reddit.columns), axis=1)\n",
    "\n",
    "selected_cols = ['title', 'created_utc', 'ups', 'downs', 'upvote_ratio', 'score', 'num_comments', 'is_original_content', 'permalink', 'url']\n",
    "\n",
    "df_reddit = df_reddit[selected_cols].copy()\n",
    "\n",
    "df_reddit.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds105",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
