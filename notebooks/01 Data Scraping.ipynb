{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÖStep 1: Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. üéØImport libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import requests as r\n",
    "\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from scrapy import Selector\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import spacy\n",
    "# import plotnine\n",
    "# import altair\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import our own modules\n",
    "sys.path.append(\"../scripts/\")\n",
    "import chadtools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üéØAuthenticate with Reddit API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a function defined in our `utils.py` script, we can authenticate with the Reddit API using our own `credentials.json` file, and get a `dict` of headers to be used in all subsequent GET requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Authorization': 'bearer eyJhbGciOiJSUzI1NiIsImtpZCI6IlNIQTI1NjpzS3dsMnlsV0VtMjVmcXhwTU40cWY4MXE2OWFFdWFyMnpLMUdhVGxjdWNZIiwidHlwIjoiSldUIn0.eyJzdWIiOiJ1c2VyIiwiZXhwIjoxNzAyMDcyODMyLjkwOTQzMSwiaWF0IjoxNzAxOTg2NDMyLjkwOTQzMSwianRpIjoiaXZvTzBHM2RMMFota1NZakFMbjVZa1pHc1lxT1ZRIiwiY2lkIjoiQmVvRVNfeUhwNDJXWXF0aUNBeHVhZyIsImxpZCI6InQyXzhwNHl1NzBrIiwiYWlkIjoidDJfOHA0eXU3MGsiLCJsY2EiOjE2MDQxNDY3NTU0MjQsInNjcCI6ImVKeUtWdEpTaWdVRUFBRF9fd056QVNjIiwiZmxvIjo5fQ.qvcL9ULBSNWezbSG8Jxn86DNjShVPDI1q8ZMElVrE7jc57qaIUmTQYadjJPCpwKfxtjx32C3D1PMWtK2MK0yDg-hloBI61KvGmYY3_qKJgOyPKNU8PhHV3RuhlZR0d-bo2gX3KC0zmqZ_QfPSqTwnjbRHQxRFEEehly0ObQDSygQOCqkWvcZk-aMaXD21JPp7pMXJZFRVprlEhAHHRK7Xgin-qTZ8nB7dkqyzpM1J9aeQCWhTFUV0oA2LaO9AVj0kUQzUKuqHGvcbSUDMaCVPI02CbUxApe11yXARz3-r-7atPFB4bIJwT51jGYfwNVhHqxfB5S4OhiKP6e3bXbejw',\n",
       " 'User-Agent': 'LSE DS105A Recipe Scraping Project by Due_Chef1909'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = chadtools.authenticate_and_get_headers()\n",
    "headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üéØSending our GET requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Prepare GET request for all Flairs + Paginate through all search results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to loop through each flair using For Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the `after` ID given by the reddit API to paginate through until the last post matching the search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2066\n"
     ]
    }
   ],
   "source": [
    "s = r.Session()\n",
    "BASE_ENDPOINT = \"https://oauth.reddit.com\"\n",
    "flair_names = ['Recipe', 'Dessert', 'Pasta', 'Poultry', 'Vegetarian', 'Drink', 'Beef', 'Pork', 'Seafood', 'Fruit\\Vegetarian']\n",
    "subreddit_name = 'recipes'\n",
    "\n",
    "\n",
    "all_data_for_all_flairs = []\n",
    "all_data_by_flair = {}\n",
    "\n",
    "\n",
    "for flair in flair_names:\n",
    "    flair_query = f'flair_name:\"{flair}\"'\n",
    "    specific_date_time = datetime(2020, 8, 31, 10, 59, 0)\n",
    "    timestamp = int(specific_date_time.timestamp())\n",
    "    params = {\n",
    "        'q': flair_query,\n",
    "        'limit': 100,\n",
    "        'restrict_sr': 1,\n",
    "        'sort': 'new',\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "    response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/search\", params=params, headers=headers)\n",
    "    # Initialize an empty list to store the data from page for the current flair\n",
    "    all_data_by_flair[flair] = []\n",
    "    \n",
    "\n",
    "    # Process the data from the first page\n",
    "    data = response.json()\n",
    "    all_data_by_flair[flair].extend(data['data']['children'])\n",
    "\n",
    "    # Page 02 and beyond\n",
    "    while 'after' in data['data'] and data['data']['after'] is not None:\n",
    "        after_id = data['data']['after']\n",
    "        params[\"after\"] = after_id\n",
    "        response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/search\", params=params, headers=headers)\n",
    "        # print(f\"Requesting Page {len(all_data_by_flair[flair]) // params['limit'] + 1}\")\n",
    "        data = response.json()\n",
    "\n",
    "        # Process the data from the current page\n",
    "        #all_data_by_flair.extend(data['data']['children'])\n",
    "        all_data_by_flair[flair].extend(data['data']['children'])\n",
    "    \n",
    "    all_data_for_all_flairs.extend(all_data_by_flair[flair])\n",
    "    \n",
    "pprint(len(all_data_for_all_flairs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üéØSaving the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Save the data as a JSON file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/all_data_for_all_flairs.json\", \"w\") as f:\n",
    "    json.dump( all_data_for_all_flairs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load the JSON file as a Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/all_data_for_all_flairs.json\", \"r\") as file:\n",
    "    posts = json.load(file)\n",
    "    s\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Create a dataframe of all posts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: @yuyaobai add some comments in the cells below to explain the code a bit more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>permalink</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Classic Tiramisu Recipe (original Italian pizz...</td>\n",
       "      <td>1.701864e+09</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0.82</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>https://reddit.com/r/recipes/comments/18c2c0q/...</td>\n",
       "      <td>https://www.diyfoodhacks.com/classic-tiramisu-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Orange Cookies üçäüß°</td>\n",
       "      <td>1.701750e+09</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>175</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>https://reddit.com/r/recipes/comments/18b3ir1/...</td>\n",
       "      <td>https://i.redd.it/37t5h7ssje4c1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stir Fry Supreme ‚Äì Chives, cashews and Shrimp</td>\n",
       "      <td>1.701695e+09</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0.91</td>\n",
       "      <td>100</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>https://reddit.com/r/recipes/comments/18ajm70/...</td>\n",
       "      <td>https://i.redd.it/6vrftswiz94c1.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sous Vide Chicken and Potatoes</td>\n",
       "      <td>1.701651e+09</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>https://reddit.com/r/recipes/comments/18a88g3/...</td>\n",
       "      <td>https://i.redd.it/rcgqae55e64c1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chicken Riggies</td>\n",
       "      <td>1.701551e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>https://reddit.com/r/recipes/comments/189d72m/...</td>\n",
       "      <td>https://i.redd.it/bn11tg3i5y3c1.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title   created_utc  ups  \\\n",
       "0  Classic Tiramisu Recipe (original Italian pizz...  1.701864e+09   19   \n",
       "1                                  Orange Cookies üçäüß°  1.701750e+09  175   \n",
       "2      Stir Fry Supreme ‚Äì Chives, cashews and Shrimp  1.701695e+09  100   \n",
       "3                     Sous Vide Chicken and Potatoes  1.701651e+09    6   \n",
       "4                                    Chicken Riggies  1.701551e+09    1   \n",
       "\n",
       "   downs  upvote_ratio  score  num_comments  is_original_content  \\\n",
       "0      0          0.82     19             4                False   \n",
       "1      0          0.97    175             6                False   \n",
       "2      0          0.91    100             9                False   \n",
       "3      0          0.88      6             1                False   \n",
       "4      0          1.00      1             1                False   \n",
       "\n",
       "                                           permalink  \\\n",
       "0  https://reddit.com/r/recipes/comments/18c2c0q/...   \n",
       "1  https://reddit.com/r/recipes/comments/18b3ir1/...   \n",
       "2  https://reddit.com/r/recipes/comments/18ajm70/...   \n",
       "3  https://reddit.com/r/recipes/comments/18a88g3/...   \n",
       "4  https://reddit.com/r/recipes/comments/189d72m/...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.diyfoodhacks.com/classic-tiramisu-...  \n",
       "1                https://i.redd.it/37t5h7ssje4c1.jpg  \n",
       "2               https://i.redd.it/6vrftswiz94c1.jpeg  \n",
       "3                https://i.redd.it/rcgqae55e64c1.jpg  \n",
       "4                https://i.redd.it/bn11tg3i5y3c1.jpg  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts = pd.json_normalize(posts, max_level=0)\n",
    "# handle NaN values in the 'data' column by replacing them with an empty dictionary.\n",
    "df_posts['data'] = df_posts['data'].apply(lambda x: {} if pd.isna(x) else x)    \n",
    "\n",
    "#explanation needed here\n",
    "df_posts = pd.concat([df_posts.drop(['data'], axis=1), pd.json_normalize(df_posts['data'])], axis=1)\n",
    "selected_cols = ['title', 'created_utc', 'ups', 'downs', 'upvote_ratio', 'score', 'num_comments', 'is_original_content', 'permalink', 'url']\n",
    "\n",
    "df_posts = df_posts[selected_cols].copy()\n",
    "df_posts['permalink'] = \"https://reddit.com\" + df_posts['permalink']     # add prefix to each permalink \n",
    "\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Remove duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identify duplicates by checking the permalink of the pages. Comment: is this step redundant? @yuyaobai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>permalink</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, created_utc, ups, downs, upvote_ratio, score, num_comments, is_original_content, permalink, url]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_posts = df_posts[df_posts.duplicated(subset='permalink', keep=False)]\n",
    "duplicate_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_posts.to_csv('../data/duplicates.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove non-English posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the data easier to analyse using NLP techniques, we will filter out posts that are not in English."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# FAILED method\n",
    "# Function to check if text is in English\n",
    "def is_english(text):\n",
    "    language, confidence = langid.classify(text)\n",
    "    return language == 'en' and confidence > 0.00000000001  # Adjust confidence threshold as needed\n",
    "\n",
    "# Apply the function to filter rows\n",
    "filtered_df = df_posts[df_posts['title'].apply(is_english)]\n",
    "\n",
    "print(filtered_df)\n",
    "print(len(filtered_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# load the English language model into spacy\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# filter the english posts by applying custom function\u001b[39;00m\n\u001b[0;32m      5\u001b[0m filtered_df_posts \u001b[38;5;241m=\u001b[39m df_posts[df_posts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(chadtools\u001b[38;5;241m.\u001b[39mis_english, model\u001b[38;5;241m=\u001b[39mnlp)]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# load the English language model into spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# filter the english posts by applying custom function\n",
    "filtered_df_posts = df_posts[df_posts['title'].apply(chadtools.is_english, model=nlp)]\n",
    "\n",
    "filtered_df_posts.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Save dataframe as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_posts.to_csv('../data/posts.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Specify the subreddit and flair\\nsubreddit_name = \\'recipes\\'\\nflair_name = \\'recipe\\'  # Change this to the desired flair\\n\\n# Reddit API endpoint for searching posts in a subreddit\\nurl = f\\'https://www.reddit.com/r/{subreddit_name}/search.json\\'\\n\\n# Define parameters for the search query\\nparams = {\\n    \\'q\\': f\\'flair_name:\"{flair_name}\"\\',\\n    \\'restrict_sr\\': \\'on\\',  # Restrict the search to the specified subreddit\\n    \\'sort\\': \\'new\\',       # Sort by new to get all posts\\n    \\'syntax\\': \\'cloudsearch\\'\\n}\\n\\n# Make the API request\\nresponse = s.get(url, params=params, headers={\\'User-agent\\': \\'your_user_agent\\'})\\n\\n# Check if the request was successful (status code 200)\\nif response.status_code == 200:\\n    # Parse the JSON response\\n    data = response.json()\\n    \\n    # Get the number of posts\\n    num_posts = data[\\'data\\'][\\'dist\\']\\n    \\n    print(f\"Number of posts with \\'{flair_name}\\' flair in r/{subreddit_name}: {num_posts}\")\\nelse:\\n    print(f\"Error: {response.status_code}\")\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Specify the subreddit and flair\n",
    "subreddit_name = 'recipes'\n",
    "flair_name = 'recipe'  # Change this to the desired flair\n",
    "\n",
    "# Reddit API endpoint for searching posts in a subreddit\n",
    "url = f'https://www.reddit.com/r/{subreddit_name}/search.json'\n",
    "\n",
    "# Define parameters for the search query\n",
    "params = {\n",
    "    'q': f'flair_name:\"{flair_name}\"',\n",
    "    'restrict_sr': 'on',  # Restrict the search to the specified subreddit\n",
    "    'sort': 'new',       # Sort by new to get all posts\n",
    "    'syntax': 'cloudsearch'\n",
    "}\n",
    "\n",
    "# Make the API request\n",
    "response = s.get(url, params=params, headers={'User-agent': 'your_user_agent'})\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    data = response.json()\n",
    "    \n",
    "    # Get the number of posts\n",
    "    num_posts = data['data']['dist']\n",
    "    \n",
    "    print(f\"Number of posts with '{flair_name}' flair in r/{subreddit_name}: {num_posts}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds105",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
