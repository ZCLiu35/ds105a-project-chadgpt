{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âœ…Step 1: Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. ðŸŽ¯Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests as r\n",
    "\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from scrapy import Selector\n",
    "\n",
    "# import plotnine\n",
    "# import altair\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import our own modules\n",
    "os.chdir(\"../\")\n",
    "import scripts.utils as u\n",
    "os.chdir(\"./notebooks/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ðŸŽ¯Authenticate with Reddit API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a function defined in our `utils.py` script, we can authenticate with the Reddit API using our own `credentials.json` file, and get a `dict` of headers to be used in all subsequent GET requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Authorization': 'bearer eyJhbGciOiJSUzI1NiIsImtpZCI6IlNIQTI1NjpzS3dsMnlsV0VtMjVmcXhwTU40cWY4MXE2OWFFdWFyMnpLMUdhVGxjdWNZIiwidHlwIjoiSldUIn0.eyJzdWIiOiJ1c2VyIiwiZXhwIjoxNzAxNTIzOTc2LjQxODYyMywiaWF0IjoxNzAxNDM3NTc2LjQxODYyMywianRpIjoiU2hQZnQzSFNBZGxrenAwbmY5ZkR6Z0J2TUc1dVlBIiwiY2lkIjoibWhUbV82eEVUNzVkOWhmWkJrS0ZYQSIsImxpZCI6InQyXzE2ZmE0MiIsImFpZCI6InQyXzE2ZmE0MiIsImxjYSI6MTQ5MDI0NzcyNzAxMSwic2NwIjoiZUp5S1Z0SlNpZ1VFQUFEX193TnpBU2MiLCJmbG8iOjl9.M7Y3QXvFuvpJkeJ6THWNhMxF0TnxBve93MGhn3Fwwx-Jg_0z_cCQnyiVTSgjrf4IqdGm2ZEBeBCMHzREtZevFwyPLAI4B7qEfHgxm0F7XKYg6MfXfEQtXEPrxATtIsrr1_D7VJOpg6-pRDo3F5djqFYnZS3ZsSTN7T_5CFPMnRRVTpcj7fQR4tBazN8iSWuzWObSfovbTjZZ8y2S8On4KvAb_IWeYxZ2WFW2Hl8A-rQO9GpvUjBHu_JAG1cpafI_U-cfXN5wYTNwUc9OKWW-ciTyE-5-HcGzgUJRSVSYFt8AGenfkV_bVml6ntFVzB1nrsvw67TgufAmlBSbfLH6qg',\n",
       " 'User-Agent': 'LSE DS105A Recipe Scraping Project by zichengliu'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = u.authenticate_and_get_headers()\n",
    "headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ðŸŽ¯Sending our first request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will limit our search to 3 posts first, to test whether our GET request works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = r.Session()\n",
    "\n",
    "BASE_ENDPOINT = \"https://oauth.reddit.com\"\n",
    "flair_name = 'Recipe'\n",
    "subreddit_name = 'recipes'\n",
    "\n",
    "params = {'q': f'flair_name:\"{flair_name}\"',\n",
    "          'limit': 3,\n",
    "          'restrict_sr': 1,\n",
    "          'sort': 'new'}\n",
    "\n",
    "response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/search\", params=params, headers=headers)\n",
    "\n",
    "# response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try paginating 3 times first, before increasing the number of page or paginating to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting Page 2\n",
      "Requesting Page 3\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the data from all pages\n",
    "all_data = []\n",
    "\n",
    "#page 01 data\n",
    "data = response.json()\n",
    "\n",
    "# Process the data from the first page\n",
    "all_data.extend(data['data']['children'])\n",
    "\n",
    "# Continue paginating until there is no more data (or paginate for a set number of times)\n",
    "\n",
    "# while data['data']['after'] is not None:\n",
    "for i in range(2):\n",
    "    after_id = data['data']['after']\n",
    "    params[\"after\"] = after_id\n",
    "    # response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/hot?limit=100&after={after_id}/\", params=params, headers=headers)\n",
    "    response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/search\", params=params, headers=headers)\n",
    "    print(f\"Requesting Page {i+2}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Process the data from the current page\n",
    "    all_data.extend(data['data']['children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ðŸŽ¯Saving the data to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/all_data_flair_is_recipe.json\", \"w\") as f:\n",
    "    json.dump(all_data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds105",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
