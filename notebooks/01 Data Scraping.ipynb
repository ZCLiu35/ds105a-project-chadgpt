{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âœ…Step 1: Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. ðŸŽ¯Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import requests as r\n",
    "\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from scrapy import Selector\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import spacy\n",
    "# import plotnine\n",
    "# import altair\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import our own modules\n",
    "sys.path.append(\"../scripts/\")\n",
    "import chadtools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ðŸŽ¯Authenticate with Reddit API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a function defined in our `utils.py` script, we can authenticate with the Reddit API using our own `credentials.json` file, and get a `dict` of headers to be used in all subsequent GET requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Authorization': 'bearer eyJhbGciOiJSUzI1NiIsImtpZCI6IlNIQTI1NjpzS3dsMnlsV0VtMjVmcXhwTU40cWY4MXE2OWFFdWFyMnpLMUdhVGxjdWNZIiwidHlwIjoiSldUIn0.eyJzdWIiOiJ1c2VyIiwiZXhwIjoxNzAyMDcyODMyLjkwOTQzMSwiaWF0IjoxNzAxOTg2NDMyLjkwOTQzMSwianRpIjoiaXZvTzBHM2RMMFota1NZakFMbjVZa1pHc1lxT1ZRIiwiY2lkIjoiQmVvRVNfeUhwNDJXWXF0aUNBeHVhZyIsImxpZCI6InQyXzhwNHl1NzBrIiwiYWlkIjoidDJfOHA0eXU3MGsiLCJsY2EiOjE2MDQxNDY3NTU0MjQsInNjcCI6ImVKeUtWdEpTaWdVRUFBRF9fd056QVNjIiwiZmxvIjo5fQ.qvcL9ULBSNWezbSG8Jxn86DNjShVPDI1q8ZMElVrE7jc57qaIUmTQYadjJPCpwKfxtjx32C3D1PMWtK2MK0yDg-hloBI61KvGmYY3_qKJgOyPKNU8PhHV3RuhlZR0d-bo2gX3KC0zmqZ_QfPSqTwnjbRHQxRFEEehly0ObQDSygQOCqkWvcZk-aMaXD21JPp7pMXJZFRVprlEhAHHRK7Xgin-qTZ8nB7dkqyzpM1J9aeQCWhTFUV0oA2LaO9AVj0kUQzUKuqHGvcbSUDMaCVPI02CbUxApe11yXARz3-r-7atPFB4bIJwT51jGYfwNVhHqxfB5S4OhiKP6e3bXbejw',\n",
       " 'User-Agent': 'LSE DS105A Recipe Scraping Project by Due_Chef1909'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = chadtools.authenticate_and_get_headers()\n",
    "headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ðŸŽ¯Sending our GET requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Prepare GET request for all Flairs + Paginate through all search results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to loop through each flair using For Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the `after` ID given by the reddit API to paginate through until the last post matching the search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2066\n"
     ]
    }
   ],
   "source": [
    "s = r.Session()\n",
    "BASE_ENDPOINT = \"https://oauth.reddit.com\"\n",
    "flair_names = ['Recipe', 'Dessert', 'Pasta', 'Poultry', 'Vegetarian', 'Drink', 'Beef', 'Pork', 'Seafood', 'Fruit\\Vegetarian']\n",
    "subreddit_name = 'recipes'\n",
    "\n",
    "\n",
    "all_data_for_all_flairs = []\n",
    "all_data_by_flair = {}\n",
    "\n",
    "\n",
    "for flair in flair_names:\n",
    "    flair_query = f'flair_name:\"{flair}\"'\n",
    "    specific_date_time = datetime(2020, 8, 31, 10, 59, 0)\n",
    "    timestamp = int(specific_date_time.timestamp())\n",
    "    params = {\n",
    "        'q': flair_query,\n",
    "        'limit': 100,\n",
    "        'restrict_sr': 1,\n",
    "        'sort': 'new',\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "    response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/search\", params=params, headers=headers)\n",
    "    # Initialize an empty list to store the data from page for the current flair\n",
    "    all_data_by_flair[flair] = []\n",
    "    \n",
    "\n",
    "    # Process the data from the first page\n",
    "    data = response.json()\n",
    "    all_data_by_flair[flair].extend(data['data']['children'])\n",
    "\n",
    "    # Page 02 and beyond\n",
    "    while 'after' in data['data'] and data['data']['after'] is not None:\n",
    "        after_id = data['data']['after']\n",
    "        params[\"after\"] = after_id\n",
    "        response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/search\", params=params, headers=headers)\n",
    "        # print(f\"Requesting Page {len(all_data_by_flair[flair]) // params['limit'] + 1}\")\n",
    "        data = response.json()\n",
    "\n",
    "        # Process the data from the current page\n",
    "        #all_data_by_flair.extend(data['data']['children'])\n",
    "        all_data_by_flair[flair].extend(data['data']['children'])\n",
    "    \n",
    "    all_data_for_all_flairs.extend(all_data_by_flair[flair])\n",
    "    \n",
    "pprint(len(all_data_for_all_flairs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ðŸŽ¯Saving the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Save the data as a JSON file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/all_data_for_all_flairs.json\", \"w\") as f:\n",
    "    json.dump( all_data_for_all_flairs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load the JSON file as a Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/all_data_for_all_flairs.json\", \"r\") as file:\n",
    "    posts = json.load(file)\n",
    "    s\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Create a dataframe of all posts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts = pd.json_normalize(posts, max_level=0)\n",
    "df_posts = pd.json_normalize(df_posts['data'])\n",
    "selected_cols = ['title', 'created_utc', 'ups', 'downs', 'upvote_ratio', 'score', 'num_comments', 'is_original_content', 'permalink', 'url']\n",
    "\n",
    "df_posts = df_posts[selected_cols].copy()\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize JSON data in 'posts' and create a DataFrame\n",
    "df_posts = pd.json_normalize(posts, max_level=0)\n",
    "\n",
    "# handle NaN values in the 'data' column by replacing them with an empty dictionary\n",
    "df_posts['data'] = df_posts['data'].apply(lambda x: {} if pd.isna(x) else x)    \n",
    "\n",
    "# concatenate the original DataFrame with a new DataFrame created from normalizing the 'data' column\n",
    "df_posts = pd.concat([df_posts.drop(['data'], axis=1), pd.json_normalize(df_posts['data'])], axis=1)\n",
    "\n",
    "# select specific columns from the dataframe\n",
    "selected_cols = ['title', 'created_utc', 'ups', 'downs', 'upvote_ratio', 'score', 'num_comments', 'is_original_content', 'permalink', 'url']\n",
    "df_posts = df_posts[selected_cols].copy()\n",
    "\n",
    "# add a prefix to the 'permalink' column\n",
    "df_posts['permalink'] = \"https://reddit.com\" + df_posts['permalink']   \n",
    "\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove non-English posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the data easier to analyse using NLP techniques, we will filter out posts that are not in English."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# FAILED method\n",
    "# Function to check if text is in English\n",
    "def is_english(text):\n",
    "    language, confidence = langid.classify(text)\n",
    "    return language == 'en' and confidence > 0.00000000001  # Adjust confidence threshold as needed\n",
    "\n",
    "# Apply the function to filter rows\n",
    "filtered_df = df_posts[df_posts['title'].apply(is_english)]\n",
    "\n",
    "print(filtered_df)\n",
    "print(len(filtered_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>permalink</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>Zurek - Polish Easter Soup</td>\n",
       "      <td>1.680433e+09</td>\n",
       "      <td>378</td>\n",
       "      <td>0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>378</td>\n",
       "      <td>18</td>\n",
       "      <td>True</td>\n",
       "      <td>https://reddit.com/r/recipes/comments/129hxb0/...</td>\n",
       "      <td>https://i.redd.it/p3b62d3mdgra1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Sweet &amp;amp; Crunchy Maple Popcorn</td>\n",
       "      <td>1.680362e+09</td>\n",
       "      <td>606</td>\n",
       "      <td>0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>606</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>https://reddit.com/r/recipes/comments/128p3mz/...</td>\n",
       "      <td>https://i.redd.it/883p3zahgara1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Matcha Fudge Chocolate Chip Marble Cookies (Re...</td>\n",
       "      <td>1.680281e+09</td>\n",
       "      <td>1405</td>\n",
       "      <td>0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1405</td>\n",
       "      <td>28</td>\n",
       "      <td>False</td>\n",
       "      <td>https://reddit.com/r/recipes/comments/127rf0v/...</td>\n",
       "      <td>https://i.redd.it/5r1vvzc7t3ra1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Greek Yogurt Chicken Salad with Apples and Alm...</td>\n",
       "      <td>1.680043e+09</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>https://reddit.com/r/recipes/comments/1254xfn/...</td>\n",
       "      <td>https://i.redd.it/kbpojb4e5kqa1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Louisiana Crawfish Boil</td>\n",
       "      <td>1.680008e+09</td>\n",
       "      <td>1215</td>\n",
       "      <td>0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1215</td>\n",
       "      <td>56</td>\n",
       "      <td>True</td>\n",
       "      <td>https://reddit.com/r/recipes/comments/124nr2l/...</td>\n",
       "      <td>https://i.redd.it/sgmiy9z18hqa1.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title   created_utc   ups  \\\n",
       "244                         Zurek - Polish Easter Soup  1.680433e+09   378   \n",
       "245                  Sweet &amp; Crunchy Maple Popcorn  1.680362e+09   606   \n",
       "246  Matcha Fudge Chocolate Chip Marble Cookies (Re...  1.680281e+09  1405   \n",
       "247  Greek Yogurt Chicken Salad with Apples and Alm...  1.680043e+09    23   \n",
       "248                            Louisiana Crawfish Boil  1.680008e+09  1215   \n",
       "\n",
       "     downs  upvote_ratio  score  num_comments  is_original_content  \\\n",
       "244      0          0.98    378            18                 True   \n",
       "245      0          0.97    606            24                 True   \n",
       "246      0          0.97   1405            28                False   \n",
       "247      0          0.97     23             2                False   \n",
       "248      0          0.96   1215            56                 True   \n",
       "\n",
       "                                             permalink  \\\n",
       "244  https://reddit.com/r/recipes/comments/129hxb0/...   \n",
       "245  https://reddit.com/r/recipes/comments/128p3mz/...   \n",
       "246  https://reddit.com/r/recipes/comments/127rf0v/...   \n",
       "247  https://reddit.com/r/recipes/comments/1254xfn/...   \n",
       "248  https://reddit.com/r/recipes/comments/124nr2l/...   \n",
       "\n",
       "                                     url  \n",
       "244  https://i.redd.it/p3b62d3mdgra1.jpg  \n",
       "245  https://i.redd.it/883p3zahgara1.jpg  \n",
       "246  https://i.redd.it/5r1vvzc7t3ra1.jpg  \n",
       "247  https://i.redd.it/kbpojb4e5kqa1.jpg  \n",
       "248  https://i.redd.it/sgmiy9z18hqa1.jpg  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the English language model into spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# filter the english posts by applying custom function\n",
    "filtered_df_posts = df_posts[df_posts['title'].apply(chadtools.is_english, model=nlp)]\n",
    "\n",
    "filtered_df_posts.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Save dataframe as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_posts.to_csv('../data/posts.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Specify the subreddit and flair\\nsubreddit_name = \\'recipes\\'\\nflair_name = \\'recipe\\'  # Change this to the desired flair\\n\\n# Reddit API endpoint for searching posts in a subreddit\\nurl = f\\'https://www.reddit.com/r/{subreddit_name}/search.json\\'\\n\\n# Define parameters for the search query\\nparams = {\\n    \\'q\\': f\\'flair_name:\"{flair_name}\"\\',\\n    \\'restrict_sr\\': \\'on\\',  # Restrict the search to the specified subreddit\\n    \\'sort\\': \\'new\\',       # Sort by new to get all posts\\n    \\'syntax\\': \\'cloudsearch\\'\\n}\\n\\n# Make the API request\\nresponse = s.get(url, params=params, headers={\\'User-agent\\': \\'your_user_agent\\'})\\n\\n# Check if the request was successful (status code 200)\\nif response.status_code == 200:\\n    # Parse the JSON response\\n    data = response.json()\\n    \\n    # Get the number of posts\\n    num_posts = data[\\'data\\'][\\'dist\\']\\n    \\n    print(f\"Number of posts with \\'{flair_name}\\' flair in r/{subreddit_name}: {num_posts}\")\\nelse:\\n    print(f\"Error: {response.status_code}\")\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Specify the subreddit and flair\n",
    "subreddit_name = 'recipes'\n",
    "flair_name = 'recipe'  # Change this to the desired flair\n",
    "\n",
    "# Reddit API endpoint for searching posts in a subreddit\n",
    "url = f'https://www.reddit.com/r/{subreddit_name}/search.json'\n",
    "\n",
    "# Define parameters for the search query\n",
    "params = {\n",
    "    'q': f'flair_name:\"{flair_name}\"',\n",
    "    'restrict_sr': 'on',  # Restrict the search to the specified subreddit\n",
    "    'sort': 'new',       # Sort by new to get all posts\n",
    "    'syntax': 'cloudsearch'\n",
    "}\n",
    "\n",
    "# Make the API request\n",
    "response = s.get(url, params=params, headers={'User-agent': 'your_user_agent'})\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    data = response.json()\n",
    "    \n",
    "    # Get the number of posts\n",
    "    num_posts = data['data']['dist']\n",
    "    \n",
    "    print(f\"Number of posts with '{flair_name}' flair in r/{subreddit_name}: {num_posts}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds105",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
