{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âœ…Step 1: Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. ðŸŽ¯Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests as r\n",
    "\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from scrapy import Selector\n",
    "\n",
    "import plotnine\n",
    "import altair\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ðŸŽ¯Load credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loads the `credentials.json` file in each of our local repos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_file_path = \"../credentials.json\"\n",
    "\n",
    "# open the file and load the data into a variable\n",
    "with open(credentials_file_path, \"r\") as f:\n",
    "    credentials = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ðŸŽ¯Obtaining a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = r.Session()\n",
    "\n",
    "# Set up authentication parameters \n",
    "client_auth = r.auth.HTTPBasicAuth(credentials[\"app_client_id\"], credentials[\"app_client_secret\"])\n",
    "\n",
    "# Send, via HTTP POST, your Reddit username and password\n",
    "post_data = {\"grant_type\": \"password\",\n",
    "             \"username\": credentials[\"reddit_username\"],\n",
    "             \"password\": credentials[\"reddit_password\"]}\n",
    "\n",
    "# Reddit API requests that we self-identify ourselves in the User-Agent\n",
    "headers = {\"User-Agent\": f\"LSE DS105A Recipe Scraping Project by {credentials['reddit_username']}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'access_token': 'eyJhbGciOiJSUzI1NiIsImtpZCI6IlNIQTI1NjpzS3dsMnlsV0VtMjVmcXhwTU40cWY4MXE2OWFFdWFyMnpLMUdhVGxjdWNZIiwidHlwIjoiSldUIn0.eyJzdWIiOiJ1c2VyIiwiZXhwIjoxNzAxNTIwNjk1LjAzNjg3NiwiaWF0IjoxNzAxNDM0Mjk1LjAzNjg3NiwianRpIjoiT3FQbTQyM3pWQWdjYVpRUThmdHhWX3o4a3A0dXlnIiwiY2lkIjoiZmVpckFYYmVWakEzOFN3cVRQT05LdyIsImxpZCI6InQyX2hleXBhN2ZhIiwiYWlkIjoidDJfaGV5cGE3ZmEiLCJsY2EiOjE2MzkxMjU5ODA5NzQsInNjcCI6ImVKeUtWdEpTaWdVRUFBRF9fd056QVNjIiwiZmxvIjo5fQ.p5P7-z5fYaD3ChRlkJWB43HlPpQCqJhUc-uLvQGJpB1bei0GYdztiy5esrmMovtqEQT_ZfDRBODpnyVrsFyCuBcqEJO2ssFgUT8djTkobGFjN1zqnVlRTi1OyPnJD_9Yo94QaO1SwdsFkeWFrRZSuuVrXceP56KT0UEkMZZimqs9O9VvYCOIOOLu58RKGW5DBSRpCv4HIRHcuUXp6MWYkzcv2g283yZ4iPQ1EDlru9g8cWOj-h1T0AnYtgwhSDn4zmBMGddnj3LnS-HWeuJ4jKClUrFKI5PMF_-pVUFMxs35VInKBpYjLMCTvrcB6rhJgFtBGgas84vIv5d-F4svqw',\n",
       " 'token_type': 'bearer',\n",
       " 'expires_in': 86400,\n",
       " 'scope': '*'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From Reddit's API documentation, this is the endpoint I need\n",
    "ACCESS_TOKEN_ENDPOINT = \"https://www.reddit.com/api/v1/access_token\"\n",
    "\n",
    "# Send a HTTP POST \n",
    "response = s.post(ACCESS_TOKEN_ENDPOINT, auth=client_auth, data=post_data, headers=headers)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save our token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_token = response.json()['access_token']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, all requests need to be followed by these HTTP HEADERS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Authorization\": f\"bearer {my_token}\",\n",
    "           \"User-Agent\": f\"LSE DS105A Recipe Scraping Project by {credentials['reddit_username']}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ðŸŽ¯Sending our first request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will limit our search to 3 posts first, to test whether our GET request works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_ENDPOINT = \"https://oauth.reddit.com\"\n",
    "flair_name = 'Recipe'\n",
    "subreddit_name = 'recipes'\n",
    "\n",
    "params = {'q': f'flair_name:\"{flair_name}\"',\n",
    "          'limit': 100,\n",
    "          'restrict_sr': 1,\n",
    "          'sort': 'new'}\n",
    "\n",
    "response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/search\", params=params, headers=headers)\n",
    "\n",
    "# response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try paginating 3 times first, before increasing the number of page or paginating to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting Page 2\n",
      "Requesting Page 3\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the data from all pages\n",
    "all_data = []\n",
    "\n",
    "#page 01 data\n",
    "data = response.json()\n",
    "\n",
    "# Process the data from the first page\n",
    "all_data.extend(data['data']['children'])\n",
    "\n",
    "# Continue paginating until there is no more data (or paginate for a set number of times)\n",
    "\n",
    "# while data['data']['after'] is not None:\n",
    "for i in range(2):\n",
    "    after_id = data['data']['after']\n",
    "    params[\"after\"] = after_id\n",
    "    # response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/hot?limit=100&after={after_id}/\", params=params, headers=headers)\n",
    "    response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/search\", params=params, headers=headers)\n",
    "    print(f\"Requesting Page {i+2}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Process the data from the current page\n",
    "    all_data.extend(data['data']['children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ðŸŽ¯Saving the data to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/all_data_flair_is_recipe.json\", \"w\") as f:\n",
    "    json.dump(all_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/all_data_flair_is_recipe.json\", \"r\") as file:\n",
    "    posts = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>permalink</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Festive Southern Jalapeno Pimento Cheese Dip</td>\n",
       "      <td>1.701263e+09</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>144</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com/r/recipes/comments/186osjd/festive_...</td>\n",
       "      <td>https://i.redd.it/i0lvs10aba3c1.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quick &amp;amp; Easy Nut Brittle</td>\n",
       "      <td>1.701206e+09</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>102</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com/r/recipes/comments/1866xrq/quick_ea...</td>\n",
       "      <td>https://i.redd.it/elfhdi81n53c1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Green Borshch</td>\n",
       "      <td>1.700952e+09</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>55</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com/r/recipes/comments/183vmzc/green_bo...</td>\n",
       "      <td>https://i.redd.it/s8aaslwrnk2c1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leftover Turkey and Pastina Soup</td>\n",
       "      <td>1.700944e+09</td>\n",
       "      <td>253</td>\n",
       "      <td>0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>253</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com/r/recipes/comments/183st4x/leftover...</td>\n",
       "      <td>https://i.redd.it/0p0rwigu0k2c1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nasi Goreng (Indonesian Fried Rice) - My famil...</td>\n",
       "      <td>1.700743e+09</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>69</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com/r/recipes/comments/1820b5b/nasi_gor...</td>\n",
       "      <td>https://i.redd.it/wj2shpc2e32c1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Greek Yogurt Chicken Salad with Apples and Alm...</td>\n",
       "      <td>1.680043e+09</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com/r/recipes/comments/1254xfn/greek_yo...</td>\n",
       "      <td>https://i.redd.it/kbpojb4e5kqa1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Louisiana Crawfish Boil</td>\n",
       "      <td>1.680008e+09</td>\n",
       "      <td>1214</td>\n",
       "      <td>0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1214</td>\n",
       "      <td>56</td>\n",
       "      <td>True</td>\n",
       "      <td>reddit.com/r/recipes/comments/124nr2l/louisian...</td>\n",
       "      <td>https://i.redd.it/sgmiy9z18hqa1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Drums of heaven</td>\n",
       "      <td>1.680000e+09</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>81</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com/r/recipes/comments/124kk3b/drums_of...</td>\n",
       "      <td>https://i.redd.it/97e0rjrm2iqa1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Butter Chicken</td>\n",
       "      <td>1.679870e+09</td>\n",
       "      <td>1444</td>\n",
       "      <td>0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1444</td>\n",
       "      <td>33</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com/r/recipes/comments/123276t/butter_c...</td>\n",
       "      <td>https://i.redd.it/tj8z88nst5qa1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Pasta with Turkey</td>\n",
       "      <td>1.679685e+09</td>\n",
       "      <td>424</td>\n",
       "      <td>0</td>\n",
       "      <td>0.93</td>\n",
       "      <td>424</td>\n",
       "      <td>17</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com/r/recipes/comments/120ucwj/pasta_wi...</td>\n",
       "      <td>https://i.redd.it/b5jmt6qs0spa1.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title   created_utc   ups  \\\n",
       "0         Festive Southern Jalapeno Pimento Cheese Dip  1.701263e+09   144   \n",
       "1                         Quick &amp; Easy Nut Brittle  1.701206e+09   102   \n",
       "2                                        Green Borshch  1.700952e+09    55   \n",
       "3                     Leftover Turkey and Pastina Soup  1.700944e+09   253   \n",
       "4    Nasi Goreng (Indonesian Fried Rice) - My famil...  1.700743e+09    69   \n",
       "..                                                 ...           ...   ...   \n",
       "245  Greek Yogurt Chicken Salad with Apples and Alm...  1.680043e+09    23   \n",
       "246                            Louisiana Crawfish Boil  1.680008e+09  1214   \n",
       "247                                    Drums of heaven  1.680000e+09    81   \n",
       "248                                     Butter Chicken  1.679870e+09  1444   \n",
       "249                                  Pasta with Turkey  1.679685e+09   424   \n",
       "\n",
       "     downs  upvote_ratio  score  num_comments  is_original_content  \\\n",
       "0        0          0.95    144             9                False   \n",
       "1        0          0.94    102            10                False   \n",
       "2        0          0.94     55             9                False   \n",
       "3        0          0.94    253            13                False   \n",
       "4        0          0.97     69            11                False   \n",
       "..     ...           ...    ...           ...                  ...   \n",
       "245      0          1.00     23             2                False   \n",
       "246      0          0.96   1214            56                 True   \n",
       "247      0          0.94     81             5                False   \n",
       "248      0          0.99   1444            33                False   \n",
       "249      0          0.93    424            17                False   \n",
       "\n",
       "                                             permalink  \\\n",
       "0    reddit.com/r/recipes/comments/186osjd/festive_...   \n",
       "1    reddit.com/r/recipes/comments/1866xrq/quick_ea...   \n",
       "2    reddit.com/r/recipes/comments/183vmzc/green_bo...   \n",
       "3    reddit.com/r/recipes/comments/183st4x/leftover...   \n",
       "4    reddit.com/r/recipes/comments/1820b5b/nasi_gor...   \n",
       "..                                                 ...   \n",
       "245  reddit.com/r/recipes/comments/1254xfn/greek_yo...   \n",
       "246  reddit.com/r/recipes/comments/124nr2l/louisian...   \n",
       "247  reddit.com/r/recipes/comments/124kk3b/drums_of...   \n",
       "248  reddit.com/r/recipes/comments/123276t/butter_c...   \n",
       "249  reddit.com/r/recipes/comments/120ucwj/pasta_wi...   \n",
       "\n",
       "                                      url  \n",
       "0    https://i.redd.it/i0lvs10aba3c1.jpeg  \n",
       "1     https://i.redd.it/elfhdi81n53c1.jpg  \n",
       "2     https://i.redd.it/s8aaslwrnk2c1.jpg  \n",
       "3     https://i.redd.it/0p0rwigu0k2c1.jpg  \n",
       "4     https://i.redd.it/wj2shpc2e32c1.jpg  \n",
       "..                                    ...  \n",
       "245   https://i.redd.it/kbpojb4e5kqa1.jpg  \n",
       "246   https://i.redd.it/sgmiy9z18hqa1.jpg  \n",
       "247   https://i.redd.it/97e0rjrm2iqa1.jpg  \n",
       "248   https://i.redd.it/tj8z88nst5qa1.jpg  \n",
       "249   https://i.redd.it/b5jmt6qs0spa1.jpg  \n",
       "\n",
       "[250 rows x 10 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts = pd.json_normalize(posts, max_level=0)\n",
    "\n",
    "df_posts['data'] = df_posts['data'].apply(lambda x: {} if pd.isna(x) else x)  # handle NaN values\n",
    "df_posts = pd.concat([df_posts.drop(['data'], axis=1), pd.json_normalize(df_posts['data'])], axis=1)\n",
    "\n",
    "selected_cols = ['title', 'created_utc', 'ups', 'downs', 'upvote_ratio', 'score', 'num_comments', 'is_original_content', 'permalink', 'url']\n",
    "\n",
    "df_posts = df_posts[selected_cols].copy()\n",
    "\n",
    "df_posts['permalink'] = df_posts['permalink'].apply(lambda x: 'reddit.com' + x)     # add prefix to each permalink \n",
    "\n",
    "df_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.to_csv('../data/posts.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds105",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
