{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âœ…Step 1: Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. ðŸŽ¯Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests as r\n",
    "\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from scrapy import Selector\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import plotnine\n",
    "import altair\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ðŸŽ¯Load credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loads the `credentials.json` file in each of our local repos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_file_path = \"../credentials.json\"\n",
    "\n",
    "# open the file and load the data into a variable\n",
    "with open(credentials_file_path, \"r\") as f:\n",
    "    credentials = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ðŸŽ¯Obtaining a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = r.Session()\n",
    "\n",
    "# Set up authentication parameters \n",
    "client_auth = r.auth.HTTPBasicAuth(credentials[\"app_client_id\"], credentials[\"app_client_secret\"])\n",
    "\n",
    "# Send, via HTTP POST, your Reddit username and password\n",
    "post_data = {\"grant_type\": \"password\",\n",
    "             \"username\": credentials[\"reddit_username\"],\n",
    "             \"password\": credentials[\"reddit_password\"]}\n",
    "\n",
    "# Reddit API requests that we self-identify ourselves in the User-Agent\n",
    "headers = {\"User-Agent\": f\"LSE DS105A Recipe Scraping Project by {credentials['reddit_username']}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'access_token': 'eyJhbGciOiJSUzI1NiIsImtpZCI6IlNIQTI1NjpzS3dsMnlsV0VtMjVmcXhwTU40cWY4MXE2OWFFdWFyMnpLMUdhVGxjdWNZIiwidHlwIjoiSldUIn0.eyJzdWIiOiJ1c2VyIiwiZXhwIjoxNzAxNzI4MTI5LjU4Nzk4MiwiaWF0IjoxNzAxNjQxNzI5LjU4Nzk4MiwianRpIjoieVc0SVB2ZWFXMWRZY2VzdGpXZVFHSklrRGpIUk1nIiwiY2lkIjoiVFJicTdUNUZLby1kTU1iSk5vMTdEQSIsImxpZCI6InQyXzJpOWF4eDh3IiwiYWlkIjoidDJfMmk5YXh4OHciLCJsY2EiOjE1NDA4OTAxODI0NTUsInNjcCI6ImVKeUtWdEpTaWdVRUFBRF9fd056QVNjIiwiZmxvIjo5fQ.mxePHDhOYMxBGhRqbYgluu3NwaOyx0TatEtVCB1HWXtLx6BtBzxMEbTJkL6EeN0eWAfqsqGhudCCd8XN2RSN25GqZEqUuERu2G_1YPPJF9Gt9NkFzg125dWJmcDD4K2f1HiO6OJChs_hWGL0GA80jydJVuhCwE4NviybvXXxOCxiOvRaAfDjRy2HBp8IcXVv3Mlc22A7Dw3XeSNnNDZ41lABK3UJmVvAJKMR13DIRKbJLQBeS51gHVyHoxodUwuWkI6Vr5PZnz7G0usARJl8Zm2c8sCq1QxbLEY0mabdb4B7T9dApHPVv3xtwkHJCOkEHd5oU_IQwDK4dilAwK_LIA',\n",
       " 'token_type': 'bearer',\n",
       " 'expires_in': 86400,\n",
       " 'scope': '*'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From Reddit's API documentation, this is the endpoint I need\n",
    "ACCESS_TOKEN_ENDPOINT = \"https://www.reddit.com/api/v1/access_token\"\n",
    "\n",
    "# Send a HTTP POST \n",
    "response = s.post(ACCESS_TOKEN_ENDPOINT, auth=client_auth, data=post_data, headers=headers)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save our token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_token = response.json()['access_token']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, all requests need to be followed by these HTTP HEADERS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Authorization\": f\"bearer {my_token}\",\n",
    "           \"User-Agent\": f\"LSE DS105A Recipe Scraping Project by {credentials['reddit_username']}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ðŸŽ¯Sending our first request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_ENDPOINT = \"https://oauth.reddit.com\"\n",
    "flair_name = ['Recipe', 'Dessert', 'Pasta', 'Poultry', 'Vegetarian', 'Drink', 'Beef', 'Pork', 'Seafood', 'Fruit\\Vegetarian']\n",
    "subreddit_name = 'recipes'\n",
    "flair_query = '|'.join(f'flair_name:\"{flair}\"' for flair in flair_name)\n",
    "specific_date_time = datetime(2020, 8, 31, 10, 59, 0)\n",
    "timestamp = int(specific_date_time.timestamp())\n",
    "\n",
    "params = {'q': flair_query,\n",
    "          'limit': 100,\n",
    "          'restrict_sr': 1,\n",
    "          'sort': 'new',\n",
    "          'timestamp': timestamp}\n",
    "\n",
    "response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/search\", params=params, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kind': 'Listing', 'data': {'modhash': None, 'dist': 0, 'facets': {}, 'after': None, 'geo_filter': '', 'children': [], 'before': None}}\n"
     ]
    }
   ],
   "source": [
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the data from all pages\n",
    "all_data_in_subreddit = []\n",
    "\n",
    "#page 01 data\n",
    "data = response.json()\n",
    "\n",
    "# Process the data from the first page\n",
    "all_data_in_subreddit.extend(data['data']['children'])\n",
    "\n",
    "# Continue paginating until there is no more data \n",
    "while 'after' in data['data'] and data['data']['after'] is not None:\n",
    "    after_id = data['data']['after']\n",
    "    params[\"after\"] = after_id\n",
    "    # response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/hot?limit=100&after={after_id}/\", params=params, headers=headers)\n",
    "    response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/search\", params=params, headers=headers)\n",
    "    print(f\"Requesting Page {len(all_data_in_subreddit) // params['limit'] + 1}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Process the data from the current page\n",
    "    all_data_in_subreddit.extend(data['data']['children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_data_in_subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will limit our search to 3 posts first, to test whether our GET request works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_ENDPOINT = \"https://oauth.reddit.com\"\n",
    "flair_name = 'Recipe'\n",
    "subreddit_name = 'recipes'\n",
    "\n",
    "params = {'q': f'flair_name:\"{flair_name}\"',\n",
    "          'limit': 100,\n",
    "          'restrict_sr': 1,\n",
    "          'sort': 'new'}\n",
    "\n",
    "response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/search\", params=params, headers=headers)\n",
    "\n",
    "# response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try paginating 3 times first, before increasing the number of page or paginating to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the data from all pages\n",
    "all_data = []\n",
    "\n",
    "#page 01 data\n",
    "data = response.json()\n",
    "\n",
    "# Process the data from the first page\n",
    "all_data.extend(data['data']['children'])\n",
    "\n",
    "# Continue paginating until there is no more data (or paginate for a set number of times)\n",
    "\n",
    "# while data['data']['after'] is not None:\n",
    "for i in range(3):\n",
    "    after_id = data['data']['after']\n",
    "    params[\"after\"] = after_id\n",
    "    # response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/hot?limit=100&after={after_id}/\", params=params, headers=headers)\n",
    "    response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/search\", params=params, headers=headers)\n",
    "    print(f\"Requesting Page {i+2}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Process the data from the current page\n",
    "    all_data.extend(data['data']['children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the data from all pages\n",
    "all_data_in_subreddit = []\n",
    "\n",
    "#page 01 data\n",
    "data = response.json()\n",
    "\n",
    "# Process the data from the first page\n",
    "all_data_in_subreddit.extend(data['data']['children'])\n",
    "\n",
    "# Continue paginating until there is no more data (or paginate for a set number of times)\n",
    "\n",
    "while 'after' in data['data'] and data['data']['after'] is not None:\n",
    "    after_id = data['data']['after']\n",
    "    params[\"after\"] = after_id\n",
    "    # response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/hot?limit=100&after={after_id}/\", params=params, headers=headers)\n",
    "    response = s.get(f\"{BASE_ENDPOINT}/r/{subreddit_name}/search\", params=params, headers=headers)\n",
    "    print(f\"Requesting Page {len(all_data_in_subreddit) // params['limit'] + 1}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Process the data from the current page\n",
    "    all_data_in_subreddit.extend(data['data']['children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_data_in_subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ðŸŽ¯Saving the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Saving the data as a JSON file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/all_data_flair_is_recipe.json\", \"w\") as f:\n",
    "    json.dump(all_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Load the JSON file as a Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/all_data_flair_is_recipe.json\", \"r\") as file:\n",
    "    posts = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Create a dataframe of all posts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts = pd.json_normalize(posts, max_level=0)\n",
    "\n",
    "df_posts['data'] = df_posts['data'].apply(lambda x: {} if pd.isna(x) else x)    # handle NaN values\n",
    "df_posts = pd.concat([df_posts.drop(['data'], axis=1), pd.json_normalize(df_posts['data'])], axis=1)\n",
    "\n",
    "selected_cols = ['title', 'created_utc', 'ups', 'downs', 'upvote_ratio', 'score', 'num_comments', 'is_original_content', 'permalink', 'url']\n",
    "\n",
    "df_posts = df_posts[selected_cols].copy()\n",
    "\n",
    "df_posts['permalink'] = df_posts['permalink'].apply(lambda x: 'reddit.com' + x)     # add prefix to each permalink \n",
    "\n",
    "df_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_posts = df_posts[df_posts.duplicated(subset='permalink', keep=False)]\n",
    "duplicate_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_posts.to_csv('../data/duplicates.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Save dataframe as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.to_csv('../data/posts.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Specify the subreddit and flair\n",
    "subreddit_name = 'recipes'\n",
    "flair_name = 'recipe'  # Change this to the desired flair\n",
    "\n",
    "# Reddit API endpoint for searching posts in a subreddit\n",
    "url = f'https://www.reddit.com/r/{subreddit_name}/search.json'\n",
    "\n",
    "# Define parameters for the search query\n",
    "params = {\n",
    "    'q': f'flair_name:\"{flair_name}\"',\n",
    "    'restrict_sr': 'on',  # Restrict the search to the specified subreddit\n",
    "    'sort': 'new',       # Sort by new to get all posts\n",
    "    'syntax': 'cloudsearch'\n",
    "}\n",
    "\n",
    "# Make the API request\n",
    "response = s.get(url, params=params, headers={'User-agent': 'your_user_agent'})\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    data = response.json()\n",
    "    \n",
    "    # Get the number of posts\n",
    "    num_posts = data['data']['dist']\n",
    "    \n",
    "    print(f\"Number of posts with '{flair_name}' flair in r/{subreddit_name}: {num_posts}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds105",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
